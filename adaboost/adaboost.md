决策树
前提概念：根节点，父节点，子节点，叶子节点，分叉，属性，标签
		  信息熵：平均不确定性
实例：白富美相亲，特征：年龄，长相，收入，公务员。标签：见和不见----------------可见决策数是监督学习
1，决策数采用的是自顶向下的递归方法
2，基本思想是以信息熵为度量，建立一个熵值下降最快的树，到叶子节点处的熵值为0
3，此时每个叶子节点中的实例都属于同一类


决策数学习的生成算法
	建立决策数的关键是当前状态下选择那个属性作为分类依据，即最优属性，由于目标函数不同，建立决策数主要有以下三种算法：ID3，C4.5，CART

ID3：找一个特别能区分标签的属性，利用信息增益（熵与条件熵的差，即互信息）遍历所有特征，选择信息增益最大的属性作为当前的分裂特征
	信息增益率：				
利用信息增益率，分得很细，增大分母，作为惩罚，从而变成C4.5
	Gini系数：将f(x)=-ln x在x=1处一阶展开，得到f(x)约= 1-x							


决策树防止过拟合两种办法：
		剪枝：根据一些理论减去多余的分支
		随机森林：随机选取特征作为根节点，构造不同的决策树，根据样本点在不同决策树中的类别，少数服从多数，选取最合适的决策树。
