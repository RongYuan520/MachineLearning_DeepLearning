{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "输入是[24,24,3]\n",
    "\n",
    "conv1:[24,24,64](filter:[5,5,3],size=64,padding=same,strides=[1,1],lambd=0)\n",
    "max_pool1:[12,12,64](ksize=[3,3],strides=[2,2])\n",
    "#norm1:[12,12,64]\n",
    "\n",
    "\n",
    "conv2:[12,12,64](filter:[5,5,64],size=64,padding=same,strides=[1,1])\n",
    "#norm2:[12,12,64]\n",
    "max_pool2:[6,6,64](ksize=[3,3],strides=[2,2])\n",
    "\n",
    "flat:\n",
    "    w_fc1\n",
    "fc1:384\n",
    "    w_fc2\n",
    "fc2:192\n",
    "    w_score\n",
    "logits:10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1，数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cifar10,cifar10_input\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "epoch = 3000\n",
    "data_dir = \"./cifar10_data/cifar-10-batches-bin\"\n",
    "#下载并解压数据集\n",
    "#cifar10.maybe_download_and_extract()\n",
    "#在data_dir路径下分批取出batch_size的训练数据\n",
    "images_train,labels_train = cifar10_input.distorted_inputs(data_dir=data_dir,batch_size=batch_size)\n",
    "#在data_dir路径下生成取出测试数据\n",
    "images_test,labels_test = cifar10_input.inputs(eval_data=True,data_dir=data_dir,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2,准备好placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_holder = tf.placeholder(tf.float32,[batch_size,24,24,3],name=\"x\")\n",
    "\n",
    "y_ = tf.placeholder(tf.int32,[batch_size],name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3,初始化参数/权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_with_weight_loss(shape,stddev,lambd):\n",
    "    weight = tf.Variable(tf.truncated_normal(shape,stddev=stddev))\n",
    "    if lambd is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(weight),lambd,name=\"weight_loss\")\n",
    "        tf.add_to_collection('losses',weight_loss)\n",
    "    return weight\n",
    "\n",
    "w_conv1 = variable_with_weight_loss(shape=[5,5,3,64],stddev=5e-2,lambd=0.0)\n",
    "b_conv1 = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "w_conv2 = variable_with_weight_loss(shape=[5,5,64,64],stddev=5e-2,lambd=0.0)\n",
    "b_conv2 = tf.Variable(tf.zeros([64]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4,拿到每个类别的score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(image_holder,w_conv1,strides=[1,1,1,1],padding=\"SAME\") + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_conv1,w_conv2,strides=[1,1,1,1],padding=\"SAME\") + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2,[batch_size,-1])\n",
    "\n",
    "dim = h_pool2_flat.get_shape()[1].value\n",
    "w_fc1 = variable_with_weight_loss(shape=[dim,384],stddev=0.04,lambd=0.004)\n",
    "b_fc1 = tf.Variable(tf.zeros([384]))\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,w_fc1) + b_fc1)\n",
    "\n",
    "w_fc2 = variable_with_weight_loss(shape=[384,192],stddev=0.04,lambd=0.004)\n",
    "b_fc2 = tf.Variable(tf.zeros([192]))\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1,w_fc2) + b_fc2)\n",
    "\n",
    "w_score = variable_with_weight_loss(shape=[192,10],stddev=1/192.0,lambd=0.0)\n",
    "b_score = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "logits = tf.matmul(h_fc2,w_score) + b_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5,计算多分类的softmax的loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交叉熵损失\n",
    "entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = y_,name=\"loss\")\n",
    "#求平均值\n",
    "loss = tf.reduce_mean(entropy)\n",
    "tf.add_to_collection('losses',loss)\n",
    "loss = tf.add_n(tf.get_collection(\"losses\"),name=\"total_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6,准备好optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7,在session里执行graph里定义的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0 loss: 20.17603302001953\n",
      "batch: 1 loss: 19.367431640625\n",
      "batch: 2 loss: 18.576431274414062\n",
      "batch: 3 loss: 17.80093002319336\n",
      "batch: 4 loss: 17.087751388549805\n",
      "batch: 5 loss: 16.417638778686523\n",
      "batch: 6 loss: 15.652765274047852\n",
      "batch: 7 loss: 15.026819229125977\n",
      "batch: 8 loss: 14.53170394897461\n",
      "batch: 9 loss: 13.86192798614502\n",
      "batch: 10 loss: 13.358083724975586\n",
      "batch: 11 loss: 12.760099411010742\n",
      "batch: 12 loss: 12.242015838623047\n",
      "batch: 13 loss: 11.68750286102295\n",
      "batch: 14 loss: 11.272640228271484\n",
      "batch: 15 loss: 10.939799308776855\n",
      "batch: 16 loss: 10.579182624816895\n",
      "batch: 17 loss: 10.189455032348633\n",
      "batch: 18 loss: 9.682180404663086\n",
      "batch: 19 loss: 9.30709171295166\n",
      "batch: 20 loss: 8.941240310668945\n",
      "batch: 21 loss: 8.67982292175293\n",
      "batch: 22 loss: 8.224300384521484\n",
      "batch: 23 loss: 7.978802680969238\n",
      "batch: 24 loss: 7.807090759277344\n",
      "batch: 25 loss: 7.390982627868652\n",
      "batch: 26 loss: 7.0941901206970215\n",
      "batch: 27 loss: 6.911914825439453\n",
      "batch: 28 loss: 6.632338523864746\n",
      "batch: 29 loss: 6.379767417907715\n",
      "batch: 30 loss: 6.203546524047852\n",
      "batch: 31 loss: 5.837668418884277\n",
      "batch: 32 loss: 5.755880832672119\n",
      "batch: 33 loss: 5.534894943237305\n",
      "batch: 34 loss: 5.293454170227051\n",
      "batch: 35 loss: 5.21898078918457\n",
      "batch: 36 loss: 5.058667182922363\n",
      "batch: 37 loss: 4.90605354309082\n",
      "batch: 38 loss: 4.760799407958984\n",
      "batch: 39 loss: 4.6669816970825195\n",
      "batch: 40 loss: 4.330749034881592\n",
      "batch: 41 loss: 4.386964321136475\n",
      "batch: 42 loss: 4.096897125244141\n",
      "batch: 43 loss: 4.098231315612793\n",
      "batch: 44 loss: 4.004454612731934\n",
      "batch: 45 loss: 3.880765914916992\n",
      "batch: 46 loss: 3.8622612953186035\n",
      "batch: 47 loss: 3.6412503719329834\n",
      "batch: 48 loss: 3.5994925498962402\n",
      "batch: 49 loss: 3.5025436878204346\n",
      "batch: 50 loss: 3.3310279846191406\n",
      "batch: 51 loss: 3.419384241104126\n",
      "batch: 52 loss: 3.3303942680358887\n",
      "batch: 53 loss: 3.245159149169922\n",
      "batch: 54 loss: 3.1565637588500977\n",
      "batch: 55 loss: 3.0816116333007812\n",
      "batch: 56 loss: 2.9399337768554688\n",
      "batch: 57 loss: 3.102189064025879\n",
      "batch: 58 loss: 2.955454111099243\n",
      "batch: 59 loss: 2.8495264053344727\n",
      "batch: 60 loss: 2.768969774246216\n",
      "batch: 61 loss: 2.701751232147217\n",
      "batch: 62 loss: 2.746969699859619\n",
      "batch: 63 loss: 2.6568803787231445\n",
      "batch: 64 loss: 2.6122241020202637\n",
      "batch: 65 loss: 2.471755027770996\n",
      "batch: 66 loss: 2.6702213287353516\n",
      "batch: 67 loss: 2.524834156036377\n",
      "batch: 68 loss: 2.638822078704834\n",
      "batch: 69 loss: 2.421175956726074\n",
      "batch: 70 loss: 2.5719070434570312\n",
      "batch: 71 loss: 2.560293197631836\n",
      "batch: 72 loss: 2.4991512298583984\n",
      "batch: 73 loss: 2.235799789428711\n",
      "batch: 74 loss: 2.3682351112365723\n",
      "batch: 75 loss: 2.364379405975342\n",
      "batch: 76 loss: 2.375175952911377\n",
      "batch: 77 loss: 2.340087413787842\n",
      "batch: 78 loss: 2.3569886684417725\n",
      "batch: 79 loss: 2.3572194576263428\n",
      "batch: 80 loss: 2.3694283962249756\n",
      "batch: 81 loss: 2.2216176986694336\n",
      "batch: 82 loss: 2.2651336193084717\n",
      "batch: 83 loss: 2.3354077339172363\n",
      "batch: 84 loss: 2.0742719173431396\n",
      "batch: 85 loss: 2.1773154735565186\n",
      "batch: 86 loss: 2.2002439498901367\n",
      "batch: 87 loss: 2.2188847064971924\n",
      "batch: 88 loss: 2.1684603691101074\n",
      "batch: 89 loss: 2.1618149280548096\n",
      "batch: 90 loss: 2.113687515258789\n",
      "batch: 91 loss: 2.290884017944336\n",
      "batch: 92 loss: 2.1646177768707275\n",
      "batch: 93 loss: 1.981593370437622\n",
      "batch: 94 loss: 2.126443386077881\n",
      "batch: 95 loss: 2.065384864807129\n",
      "batch: 96 loss: 2.128765106201172\n",
      "batch: 97 loss: 2.1114118099212646\n",
      "batch: 98 loss: 2.020728349685669\n",
      "batch: 99 loss: 2.1409401893615723\n",
      "batch: 100 loss: 2.0721359252929688\n",
      "batch: 101 loss: 2.0700173377990723\n",
      "batch: 102 loss: 1.8571373224258423\n",
      "batch: 103 loss: 2.0465948581695557\n",
      "batch: 104 loss: 2.09771990776062\n",
      "batch: 105 loss: 2.067946672439575\n",
      "batch: 106 loss: 1.9731650352478027\n",
      "batch: 107 loss: 1.9326847791671753\n",
      "batch: 108 loss: 2.028353691101074\n",
      "batch: 109 loss: 2.031292200088501\n",
      "batch: 110 loss: 2.0678751468658447\n",
      "batch: 111 loss: 1.9055216312408447\n",
      "batch: 112 loss: 1.7538890838623047\n",
      "batch: 113 loss: 1.8960845470428467\n",
      "batch: 114 loss: 2.065457344055176\n",
      "batch: 115 loss: 1.8618965148925781\n",
      "batch: 116 loss: 1.9850709438323975\n",
      "batch: 117 loss: 1.962280511856079\n",
      "batch: 118 loss: 1.974960446357727\n",
      "batch: 119 loss: 2.023850440979004\n",
      "batch: 120 loss: 1.8773709535598755\n",
      "batch: 121 loss: 1.8441039323806763\n",
      "batch: 122 loss: 1.9042773246765137\n",
      "batch: 123 loss: 1.781792402267456\n",
      "batch: 124 loss: 2.069831371307373\n",
      "batch: 125 loss: 1.954387903213501\n",
      "batch: 126 loss: 1.8674936294555664\n",
      "batch: 127 loss: 2.114140272140503\n",
      "batch: 128 loss: 1.8789772987365723\n",
      "batch: 129 loss: 1.7799854278564453\n",
      "batch: 130 loss: 1.973847508430481\n",
      "batch: 131 loss: 1.7658618688583374\n",
      "batch: 132 loss: 1.8914010524749756\n",
      "batch: 133 loss: 1.8600443601608276\n",
      "batch: 134 loss: 1.7841041088104248\n",
      "batch: 135 loss: 1.9659316539764404\n",
      "batch: 136 loss: 1.95892333984375\n",
      "batch: 137 loss: 1.6577246189117432\n",
      "batch: 138 loss: 1.8867430686950684\n",
      "batch: 139 loss: 1.86794912815094\n",
      "batch: 140 loss: 1.8869714736938477\n",
      "batch: 141 loss: 1.8653817176818848\n",
      "batch: 142 loss: 1.9322956800460815\n",
      "batch: 143 loss: 1.9293222427368164\n",
      "batch: 144 loss: 1.8410533666610718\n",
      "batch: 145 loss: 1.8162310123443604\n",
      "batch: 146 loss: 1.7212529182434082\n",
      "batch: 147 loss: 1.8047906160354614\n",
      "batch: 148 loss: 1.8000117540359497\n",
      "batch: 149 loss: 1.817415475845337\n",
      "batch: 150 loss: 1.9623808860778809\n",
      "batch: 151 loss: 1.8036813735961914\n",
      "batch: 152 loss: 1.7896904945373535\n",
      "batch: 153 loss: 1.7920711040496826\n",
      "batch: 154 loss: 1.7140891551971436\n",
      "batch: 155 loss: 1.9162449836730957\n",
      "batch: 156 loss: 1.7561825513839722\n",
      "batch: 157 loss: 1.8796344995498657\n",
      "batch: 158 loss: 2.0462729930877686\n",
      "batch: 159 loss: 1.7566505670547485\n",
      "batch: 160 loss: 1.8865063190460205\n",
      "batch: 161 loss: 1.7914526462554932\n",
      "batch: 162 loss: 1.9746735095977783\n",
      "batch: 163 loss: 1.8761218786239624\n",
      "batch: 164 loss: 1.8049156665802002\n",
      "batch: 165 loss: 1.8699311017990112\n",
      "batch: 166 loss: 1.780765414237976\n",
      "batch: 167 loss: 1.8412001132965088\n",
      "batch: 168 loss: 1.9516750574111938\n",
      "batch: 169 loss: 1.745473027229309\n",
      "batch: 170 loss: 1.8041822910308838\n",
      "batch: 171 loss: 1.978171944618225\n",
      "batch: 172 loss: 1.806778073310852\n",
      "batch: 173 loss: 1.6083201169967651\n",
      "batch: 174 loss: 1.7008099555969238\n",
      "batch: 175 loss: 1.8811500072479248\n",
      "batch: 176 loss: 1.8382525444030762\n",
      "batch: 177 loss: 1.886125922203064\n",
      "batch: 178 loss: 1.8143073320388794\n",
      "batch: 179 loss: 1.6700999736785889\n",
      "batch: 180 loss: 1.7192519903182983\n",
      "batch: 181 loss: 1.770832896232605\n",
      "batch: 182 loss: 1.837175965309143\n",
      "batch: 183 loss: 1.6354296207427979\n",
      "batch: 184 loss: 1.7840644121170044\n",
      "batch: 185 loss: 1.7816940546035767\n",
      "batch: 186 loss: 1.9038008451461792\n",
      "batch: 187 loss: 1.767440915107727\n",
      "batch: 188 loss: 1.5550549030303955\n",
      "batch: 189 loss: 1.676216959953308\n",
      "batch: 190 loss: 1.9662011861801147\n",
      "batch: 191 loss: 1.7247307300567627\n",
      "batch: 192 loss: 1.8869292736053467\n",
      "batch: 193 loss: 1.8454530239105225\n",
      "batch: 194 loss: 1.7204127311706543\n",
      "batch: 195 loss: 1.7245317697525024\n",
      "batch: 196 loss: 1.6630680561065674\n",
      "batch: 197 loss: 1.8045588731765747\n",
      "batch: 198 loss: 1.8978404998779297\n",
      "batch: 199 loss: 1.6851661205291748\n",
      "batch: 200 loss: 1.9629929065704346\n",
      "batch: 201 loss: 1.8442121744155884\n",
      "batch: 202 loss: 1.7672338485717773\n",
      "batch: 203 loss: 1.7046566009521484\n",
      "batch: 204 loss: 1.893792748451233\n",
      "batch: 205 loss: 1.6382277011871338\n",
      "batch: 206 loss: 1.9707183837890625\n",
      "batch: 207 loss: 1.7544950246810913\n",
      "batch: 208 loss: 1.705885410308838\n",
      "batch: 209 loss: 1.7349871397018433\n",
      "batch: 210 loss: 1.879559874534607\n",
      "batch: 211 loss: 1.8549145460128784\n",
      "batch: 212 loss: 1.7647734880447388\n",
      "batch: 213 loss: 1.5514417886734009\n",
      "batch: 214 loss: 1.7537987232208252\n",
      "batch: 215 loss: 1.7465581893920898\n",
      "batch: 216 loss: 1.7086496353149414\n",
      "batch: 217 loss: 1.7103639841079712\n",
      "batch: 218 loss: 1.8150650262832642\n",
      "batch: 219 loss: 1.7737658023834229\n",
      "batch: 220 loss: 1.7986021041870117\n",
      "batch: 221 loss: 1.7645504474639893\n",
      "batch: 222 loss: 1.677966594696045\n",
      "batch: 223 loss: 1.6900789737701416\n",
      "batch: 224 loss: 1.6027593612670898\n",
      "batch: 225 loss: 1.7054188251495361\n",
      "batch: 226 loss: 1.6161335706710815\n",
      "batch: 227 loss: 1.7422077655792236\n",
      "batch: 228 loss: 1.7011796236038208\n",
      "batch: 229 loss: 1.7106431722640991\n",
      "batch: 230 loss: 1.8403230905532837\n",
      "batch: 231 loss: 1.835820198059082\n",
      "batch: 232 loss: 1.710375189781189\n",
      "batch: 233 loss: 1.7406426668167114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 234 loss: 1.8413975238800049\n",
      "batch: 235 loss: 1.6564396619796753\n",
      "batch: 236 loss: 1.83003568649292\n",
      "batch: 237 loss: 1.8226819038391113\n",
      "batch: 238 loss: 1.7417874336242676\n",
      "batch: 239 loss: 1.7161099910736084\n",
      "batch: 240 loss: 1.6571096181869507\n",
      "batch: 241 loss: 1.8155174255371094\n",
      "batch: 242 loss: 1.7383010387420654\n",
      "batch: 243 loss: 1.6058412790298462\n",
      "batch: 244 loss: 1.6264851093292236\n",
      "batch: 245 loss: 1.7932682037353516\n",
      "batch: 246 loss: 1.7633382081985474\n",
      "batch: 247 loss: 1.6924229860305786\n",
      "batch: 248 loss: 1.586107850074768\n",
      "batch: 249 loss: 1.841901183128357\n",
      "batch: 250 loss: 1.6939759254455566\n",
      "batch: 251 loss: 1.8210827112197876\n",
      "batch: 252 loss: 1.7470489740371704\n",
      "batch: 253 loss: 1.8185628652572632\n",
      "batch: 254 loss: 1.6998122930526733\n",
      "batch: 255 loss: 1.9413414001464844\n",
      "batch: 256 loss: 1.8147732019424438\n",
      "batch: 257 loss: 1.8738059997558594\n",
      "batch: 258 loss: 1.7190951108932495\n",
      "batch: 259 loss: 1.7101304531097412\n",
      "batch: 260 loss: 1.8186043500900269\n",
      "batch: 261 loss: 1.886544942855835\n",
      "batch: 262 loss: 1.882643461227417\n",
      "batch: 263 loss: 1.7468088865280151\n",
      "batch: 264 loss: 1.7088425159454346\n",
      "batch: 265 loss: 1.606217622756958\n",
      "batch: 266 loss: 1.6943895816802979\n",
      "batch: 267 loss: 1.7215746641159058\n",
      "batch: 268 loss: 1.7278193235397339\n",
      "batch: 269 loss: 1.823193907737732\n",
      "batch: 270 loss: 1.7462846040725708\n",
      "batch: 271 loss: 1.923245906829834\n",
      "batch: 272 loss: 1.8191192150115967\n",
      "batch: 273 loss: 1.774344801902771\n",
      "batch: 274 loss: 1.5778756141662598\n",
      "batch: 275 loss: 1.7305928468704224\n",
      "batch: 276 loss: 1.7661199569702148\n",
      "batch: 277 loss: 1.6905025243759155\n",
      "batch: 278 loss: 1.8078088760375977\n",
      "batch: 279 loss: 1.7070918083190918\n",
      "batch: 280 loss: 1.7460777759552002\n",
      "batch: 281 loss: 1.6372686624526978\n",
      "batch: 282 loss: 1.6815816164016724\n",
      "batch: 283 loss: 1.756587266921997\n",
      "batch: 284 loss: 1.6192747354507446\n",
      "batch: 285 loss: 1.6622732877731323\n",
      "batch: 286 loss: 1.6330221891403198\n",
      "batch: 287 loss: 1.7643635272979736\n",
      "batch: 288 loss: 1.7843724489212036\n",
      "batch: 289 loss: 1.6862764358520508\n",
      "batch: 290 loss: 1.7611943483352661\n",
      "batch: 291 loss: 1.7510905265808105\n",
      "batch: 292 loss: 1.8886584043502808\n",
      "batch: 293 loss: 1.6696107387542725\n",
      "batch: 294 loss: 1.605692744255066\n",
      "batch: 295 loss: 1.8467200994491577\n",
      "batch: 296 loss: 1.816683053970337\n",
      "batch: 297 loss: 1.6334798336029053\n",
      "batch: 298 loss: 1.6132385730743408\n",
      "batch: 299 loss: 1.5922021865844727\n",
      "batch: 300 loss: 1.5371828079223633\n",
      "batch: 301 loss: 1.6623363494873047\n",
      "batch: 302 loss: 1.6015360355377197\n",
      "batch: 303 loss: 1.6395121812820435\n",
      "batch: 304 loss: 1.7965221405029297\n",
      "batch: 305 loss: 1.6707748174667358\n",
      "batch: 306 loss: 1.6348097324371338\n",
      "batch: 307 loss: 1.6085025072097778\n",
      "batch: 308 loss: 1.7383124828338623\n",
      "batch: 309 loss: 1.640921950340271\n",
      "batch: 310 loss: 1.7135205268859863\n",
      "batch: 311 loss: 1.583462119102478\n",
      "batch: 312 loss: 1.5154720544815063\n",
      "batch: 313 loss: 1.5094778537750244\n",
      "batch: 314 loss: 1.5336569547653198\n",
      "batch: 315 loss: 1.7241392135620117\n",
      "batch: 316 loss: 1.647958755493164\n",
      "batch: 317 loss: 1.7247328758239746\n",
      "batch: 318 loss: 1.8545329570770264\n",
      "batch: 319 loss: 1.7892897129058838\n",
      "batch: 320 loss: 1.7082839012145996\n",
      "batch: 321 loss: 1.7173149585723877\n",
      "batch: 322 loss: 1.7048015594482422\n",
      "batch: 323 loss: 1.5995659828186035\n",
      "batch: 324 loss: 1.6713857650756836\n",
      "batch: 325 loss: 1.7189834117889404\n",
      "batch: 326 loss: 1.6754893064498901\n",
      "batch: 327 loss: 1.6905310153961182\n",
      "batch: 328 loss: 1.814612627029419\n",
      "batch: 329 loss: 1.780872106552124\n",
      "batch: 330 loss: 1.6236149072647095\n",
      "batch: 331 loss: 1.670069694519043\n",
      "batch: 332 loss: 1.747654914855957\n",
      "batch: 333 loss: 1.67119300365448\n",
      "batch: 334 loss: 1.650527834892273\n",
      "batch: 335 loss: 1.5782049894332886\n",
      "batch: 336 loss: 1.8464100360870361\n",
      "batch: 337 loss: 1.749246597290039\n",
      "batch: 338 loss: 1.6838419437408447\n",
      "batch: 339 loss: 1.7352935075759888\n",
      "batch: 340 loss: 1.7181001901626587\n",
      "batch: 341 loss: 1.6339812278747559\n",
      "batch: 342 loss: 1.6456329822540283\n",
      "batch: 343 loss: 1.6641112565994263\n",
      "batch: 344 loss: 1.6981165409088135\n",
      "batch: 345 loss: 1.6901695728302002\n",
      "batch: 346 loss: 1.6291018724441528\n",
      "batch: 347 loss: 1.820487380027771\n",
      "batch: 348 loss: 1.7393970489501953\n",
      "batch: 349 loss: 1.7454320192337036\n",
      "batch: 350 loss: 1.5961978435516357\n",
      "batch: 351 loss: 1.657012701034546\n",
      "batch: 352 loss: 1.610436201095581\n",
      "batch: 353 loss: 1.7009044885635376\n",
      "batch: 354 loss: 1.7605787515640259\n",
      "batch: 355 loss: 1.8479472398757935\n",
      "batch: 356 loss: 1.5305652618408203\n",
      "batch: 357 loss: 1.59225332736969\n",
      "batch: 358 loss: 1.6033071279525757\n",
      "batch: 359 loss: 1.7646890878677368\n",
      "batch: 360 loss: 1.7916104793548584\n",
      "batch: 361 loss: 1.4926860332489014\n",
      "batch: 362 loss: 1.5944421291351318\n",
      "batch: 363 loss: 1.5742331743240356\n",
      "batch: 364 loss: 1.5613352060317993\n",
      "batch: 365 loss: 1.647950530052185\n",
      "batch: 366 loss: 1.713424563407898\n",
      "batch: 367 loss: 1.6929277181625366\n",
      "batch: 368 loss: 1.6353518962860107\n",
      "batch: 369 loss: 1.528815507888794\n",
      "batch: 370 loss: 1.5172609090805054\n",
      "batch: 371 loss: 1.8118340969085693\n",
      "batch: 372 loss: 1.5650831460952759\n",
      "batch: 373 loss: 1.7693872451782227\n",
      "batch: 374 loss: 1.7407736778259277\n",
      "batch: 375 loss: 1.4776105880737305\n",
      "batch: 376 loss: 1.6187899112701416\n",
      "batch: 377 loss: 1.6706624031066895\n",
      "batch: 378 loss: 1.6018221378326416\n",
      "batch: 379 loss: 1.5950366258621216\n",
      "batch: 380 loss: 1.574918508529663\n",
      "batch: 381 loss: 1.4516792297363281\n",
      "batch: 382 loss: 1.5477473735809326\n",
      "batch: 383 loss: 1.6509320735931396\n",
      "batch: 384 loss: 1.6098911762237549\n",
      "batch: 385 loss: 1.7169837951660156\n",
      "batch: 386 loss: 1.6087645292282104\n",
      "batch: 387 loss: 1.6190217733383179\n",
      "batch: 388 loss: 1.6684644222259521\n",
      "batch: 389 loss: 1.6209510564804077\n",
      "batch: 390 loss: 1.462713360786438\n",
      "batch: 391 loss: 1.7759559154510498\n",
      "batch: 392 loss: 1.5956226587295532\n",
      "batch: 393 loss: 1.6373271942138672\n",
      "batch: 394 loss: 1.621856451034546\n",
      "batch: 395 loss: 1.7171269655227661\n",
      "batch: 396 loss: 1.7951221466064453\n",
      "batch: 397 loss: 1.6066595315933228\n",
      "batch: 398 loss: 1.6740862131118774\n",
      "batch: 399 loss: 1.7415977716445923\n",
      "batch: 400 loss: 1.5616204738616943\n",
      "batch: 401 loss: 1.7710909843444824\n",
      "batch: 402 loss: 1.6481635570526123\n",
      "batch: 403 loss: 1.6295971870422363\n",
      "batch: 404 loss: 1.4809259176254272\n",
      "batch: 405 loss: 1.7029602527618408\n",
      "batch: 406 loss: 1.6623871326446533\n",
      "batch: 407 loss: 1.605359673500061\n",
      "batch: 408 loss: 1.6539406776428223\n",
      "batch: 409 loss: 1.4093091487884521\n",
      "batch: 410 loss: 1.6645172834396362\n",
      "batch: 411 loss: 1.4914298057556152\n",
      "batch: 412 loss: 1.6710046529769897\n",
      "batch: 413 loss: 1.5328397750854492\n",
      "batch: 414 loss: 1.8866145610809326\n",
      "batch: 415 loss: 1.4741034507751465\n",
      "batch: 416 loss: 1.577388048171997\n",
      "batch: 417 loss: 1.6012405157089233\n",
      "batch: 418 loss: 1.5966408252716064\n",
      "batch: 419 loss: 1.5199929475784302\n",
      "batch: 420 loss: 1.5295395851135254\n",
      "batch: 421 loss: 1.7123621702194214\n",
      "batch: 422 loss: 1.6648054122924805\n",
      "batch: 423 loss: 1.8022115230560303\n",
      "batch: 424 loss: 1.5639081001281738\n",
      "batch: 425 loss: 1.6040533781051636\n",
      "batch: 426 loss: 1.6316890716552734\n",
      "batch: 427 loss: 1.5845807790756226\n",
      "batch: 428 loss: 1.5755382776260376\n",
      "batch: 429 loss: 1.657854676246643\n",
      "batch: 430 loss: 1.5937622785568237\n",
      "batch: 431 loss: 1.792802095413208\n",
      "batch: 432 loss: 1.5685327053070068\n",
      "batch: 433 loss: 1.6099765300750732\n",
      "batch: 434 loss: 1.7157492637634277\n",
      "batch: 435 loss: 1.5716630220413208\n",
      "batch: 436 loss: 1.5133771896362305\n",
      "batch: 437 loss: 1.6377744674682617\n",
      "batch: 438 loss: 1.5690335035324097\n",
      "batch: 439 loss: 1.615324854850769\n",
      "batch: 440 loss: 1.6049617528915405\n",
      "batch: 441 loss: 1.519351601600647\n",
      "batch: 442 loss: 1.633461356163025\n",
      "batch: 443 loss: 1.6007031202316284\n",
      "batch: 444 loss: 1.5724644660949707\n",
      "batch: 445 loss: 1.5610055923461914\n",
      "batch: 446 loss: 1.5352551937103271\n",
      "batch: 447 loss: 1.563442587852478\n",
      "batch: 448 loss: 1.6456289291381836\n",
      "batch: 449 loss: 1.6632245779037476\n",
      "batch: 450 loss: 1.5724010467529297\n",
      "batch: 451 loss: 1.580361008644104\n",
      "batch: 452 loss: 1.6791040897369385\n",
      "batch: 453 loss: 1.5676593780517578\n",
      "batch: 454 loss: 1.6911945343017578\n",
      "batch: 455 loss: 1.5059139728546143\n",
      "batch: 456 loss: 1.6654788255691528\n",
      "batch: 457 loss: 1.6543937921524048\n",
      "batch: 458 loss: 1.5632802248001099\n",
      "batch: 459 loss: 1.6527059078216553\n",
      "batch: 460 loss: 1.563749074935913\n",
      "batch: 461 loss: 1.555440902709961\n",
      "batch: 462 loss: 1.764495849609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 463 loss: 1.6296470165252686\n",
      "batch: 464 loss: 1.4841922521591187\n",
      "batch: 465 loss: 1.5310322046279907\n",
      "batch: 466 loss: 1.5946239233016968\n",
      "batch: 467 loss: 1.5329930782318115\n",
      "batch: 468 loss: 1.7170560359954834\n",
      "batch: 469 loss: 1.6003705263137817\n",
      "batch: 470 loss: 1.5847256183624268\n",
      "batch: 471 loss: 1.622113823890686\n",
      "batch: 472 loss: 1.4226974248886108\n",
      "batch: 473 loss: 1.7246901988983154\n",
      "batch: 474 loss: 1.5835545063018799\n",
      "batch: 475 loss: 1.8301239013671875\n",
      "batch: 476 loss: 1.545778751373291\n",
      "batch: 477 loss: 1.8017330169677734\n",
      "batch: 478 loss: 1.6056160926818848\n",
      "batch: 479 loss: 1.620112419128418\n",
      "batch: 480 loss: 1.4903028011322021\n",
      "batch: 481 loss: 1.5614557266235352\n",
      "batch: 482 loss: 1.6855207681655884\n",
      "batch: 483 loss: 1.539716362953186\n",
      "batch: 484 loss: 1.5196537971496582\n",
      "batch: 485 loss: 1.546643853187561\n",
      "batch: 486 loss: 1.7413477897644043\n",
      "batch: 487 loss: 1.5508308410644531\n",
      "batch: 488 loss: 1.3606560230255127\n",
      "batch: 489 loss: 1.7524982690811157\n",
      "batch: 490 loss: 1.5770214796066284\n",
      "batch: 491 loss: 1.6341805458068848\n",
      "batch: 492 loss: 1.6727632284164429\n",
      "batch: 493 loss: 1.6637481451034546\n",
      "batch: 494 loss: 1.6512657403945923\n",
      "batch: 495 loss: 1.4568794965744019\n",
      "batch: 496 loss: 1.6085513830184937\n",
      "batch: 497 loss: 1.5296651124954224\n",
      "batch: 498 loss: 1.5829474925994873\n",
      "batch: 499 loss: 1.6777855157852173\n",
      "batch: 500 loss: 1.6329282522201538\n",
      "batch: 501 loss: 1.5568991899490356\n",
      "batch: 502 loss: 1.5692862272262573\n",
      "batch: 503 loss: 1.498629093170166\n",
      "batch: 504 loss: 1.6869267225265503\n",
      "batch: 505 loss: 1.5511385202407837\n",
      "batch: 506 loss: 1.4436676502227783\n",
      "batch: 507 loss: 1.5787897109985352\n",
      "batch: 508 loss: 1.6307214498519897\n",
      "batch: 509 loss: 1.6605373620986938\n",
      "batch: 510 loss: 1.5900720357894897\n",
      "batch: 511 loss: 1.5326521396636963\n",
      "batch: 512 loss: 1.5724740028381348\n",
      "batch: 513 loss: 1.677117109298706\n",
      "batch: 514 loss: 1.4882882833480835\n",
      "batch: 515 loss: 1.5040347576141357\n",
      "batch: 516 loss: 1.5364911556243896\n",
      "batch: 517 loss: 1.517034888267517\n",
      "batch: 518 loss: 1.5169178247451782\n",
      "batch: 519 loss: 1.497946858406067\n",
      "batch: 520 loss: 1.4865503311157227\n",
      "batch: 521 loss: 1.621199369430542\n",
      "batch: 522 loss: 1.5124194622039795\n",
      "batch: 523 loss: 1.5651204586029053\n",
      "batch: 524 loss: 1.6753759384155273\n",
      "batch: 525 loss: 1.5638960599899292\n",
      "batch: 526 loss: 1.3229960203170776\n",
      "batch: 527 loss: 1.48489248752594\n",
      "batch: 528 loss: 1.5636374950408936\n",
      "batch: 529 loss: 1.6998976469039917\n",
      "batch: 530 loss: 1.5384289026260376\n",
      "batch: 531 loss: 1.484513282775879\n",
      "batch: 532 loss: 1.4917137622833252\n",
      "batch: 533 loss: 1.5132102966308594\n",
      "batch: 534 loss: 1.671359896659851\n",
      "batch: 535 loss: 1.544907808303833\n",
      "batch: 536 loss: 1.6600168943405151\n",
      "batch: 537 loss: 1.5118404626846313\n",
      "batch: 538 loss: 1.658929705619812\n",
      "batch: 539 loss: 1.7055834531784058\n",
      "batch: 540 loss: 1.494876742362976\n",
      "batch: 541 loss: 1.6109927892684937\n",
      "batch: 542 loss: 1.5413590669631958\n",
      "batch: 543 loss: 1.5358750820159912\n",
      "batch: 544 loss: 1.5995802879333496\n",
      "batch: 545 loss: 1.4944400787353516\n",
      "batch: 546 loss: 1.4640312194824219\n",
      "batch: 547 loss: 1.5053590536117554\n",
      "batch: 548 loss: 1.5200238227844238\n",
      "batch: 549 loss: 1.6169626712799072\n",
      "batch: 550 loss: 1.5879640579223633\n",
      "batch: 551 loss: 1.5668748617172241\n",
      "batch: 552 loss: 1.7201545238494873\n",
      "batch: 553 loss: 1.7345792055130005\n",
      "batch: 554 loss: 1.5253896713256836\n",
      "batch: 555 loss: 1.4589014053344727\n",
      "batch: 556 loss: 1.5531336069107056\n",
      "batch: 557 loss: 1.5404366254806519\n",
      "batch: 558 loss: 1.5044622421264648\n",
      "batch: 559 loss: 1.394498586654663\n",
      "batch: 560 loss: 1.465219259262085\n",
      "batch: 561 loss: 1.6044249534606934\n",
      "batch: 562 loss: 1.582539439201355\n",
      "batch: 563 loss: 1.5322808027267456\n",
      "batch: 564 loss: 1.4832097291946411\n",
      "batch: 565 loss: 1.5832756757736206\n",
      "batch: 566 loss: 1.6779899597167969\n",
      "batch: 567 loss: 1.679236888885498\n",
      "batch: 568 loss: 1.6927061080932617\n",
      "batch: 569 loss: 1.5265880823135376\n",
      "batch: 570 loss: 1.4430903196334839\n",
      "batch: 571 loss: 1.7710543870925903\n",
      "batch: 572 loss: 1.6791489124298096\n",
      "batch: 573 loss: 1.5735350847244263\n",
      "batch: 574 loss: 1.4947165250778198\n",
      "batch: 575 loss: 1.531264066696167\n",
      "batch: 576 loss: 1.616220235824585\n",
      "batch: 577 loss: 1.6012448072433472\n",
      "batch: 578 loss: 1.4934149980545044\n",
      "batch: 579 loss: 1.5183284282684326\n",
      "batch: 580 loss: 1.5988469123840332\n",
      "batch: 581 loss: 1.5037249326705933\n",
      "batch: 582 loss: 1.56461763381958\n",
      "batch: 583 loss: 1.4562574625015259\n",
      "batch: 584 loss: 1.5549445152282715\n",
      "batch: 585 loss: 1.4888262748718262\n",
      "batch: 586 loss: 1.6282254457473755\n",
      "batch: 587 loss: 1.5528223514556885\n",
      "batch: 588 loss: 1.4841382503509521\n",
      "batch: 589 loss: 1.6673521995544434\n",
      "batch: 590 loss: 1.5200661420822144\n",
      "batch: 591 loss: 1.5534414052963257\n",
      "batch: 592 loss: 1.4878045320510864\n",
      "batch: 593 loss: 1.470219373703003\n",
      "batch: 594 loss: 1.4803357124328613\n",
      "batch: 595 loss: 1.3953025341033936\n",
      "batch: 596 loss: 1.4319484233856201\n",
      "batch: 597 loss: 1.3918203115463257\n",
      "batch: 598 loss: 1.3929450511932373\n",
      "batch: 599 loss: 1.491302251815796\n",
      "batch: 600 loss: 1.4143555164337158\n",
      "batch: 601 loss: 1.6825463771820068\n",
      "batch: 602 loss: 1.4886215925216675\n",
      "batch: 603 loss: 1.490957260131836\n",
      "batch: 604 loss: 1.4720454216003418\n",
      "batch: 605 loss: 1.3236106634140015\n",
      "batch: 606 loss: 1.3978962898254395\n",
      "batch: 607 loss: 1.7110540866851807\n",
      "batch: 608 loss: 1.5337895154953003\n",
      "batch: 609 loss: 1.5056226253509521\n",
      "batch: 610 loss: 1.5456277132034302\n",
      "batch: 611 loss: 1.3226687908172607\n",
      "batch: 612 loss: 1.402877926826477\n",
      "batch: 613 loss: 1.4411410093307495\n",
      "batch: 614 loss: 1.3688604831695557\n",
      "batch: 615 loss: 1.5880529880523682\n",
      "batch: 616 loss: 1.454721450805664\n",
      "batch: 617 loss: 1.6331021785736084\n",
      "batch: 618 loss: 1.5092580318450928\n",
      "batch: 619 loss: 1.4993623495101929\n",
      "batch: 620 loss: 1.5044431686401367\n",
      "batch: 621 loss: 1.7353371381759644\n",
      "batch: 622 loss: 1.4560790061950684\n",
      "batch: 623 loss: 1.4505516290664673\n",
      "batch: 624 loss: 1.3855534791946411\n",
      "batch: 625 loss: 1.691550374031067\n",
      "batch: 626 loss: 1.6178851127624512\n",
      "batch: 627 loss: 1.4762345552444458\n",
      "batch: 628 loss: 1.6771223545074463\n",
      "batch: 629 loss: 1.8158518075942993\n",
      "batch: 630 loss: 1.6265054941177368\n",
      "batch: 631 loss: 1.5553340911865234\n",
      "batch: 632 loss: 1.5840468406677246\n",
      "batch: 633 loss: 1.3941001892089844\n",
      "batch: 634 loss: 1.6145620346069336\n",
      "batch: 635 loss: 1.5292510986328125\n",
      "batch: 636 loss: 1.492577075958252\n",
      "batch: 637 loss: 1.4656718969345093\n",
      "batch: 638 loss: 1.5730319023132324\n",
      "batch: 639 loss: 1.5006210803985596\n",
      "batch: 640 loss: 1.4752700328826904\n",
      "batch: 641 loss: 1.6713210344314575\n",
      "batch: 642 loss: 1.4773658514022827\n",
      "batch: 643 loss: 1.4799474477767944\n",
      "batch: 644 loss: 1.4938048124313354\n",
      "batch: 645 loss: 1.5342477560043335\n",
      "batch: 646 loss: 1.5214803218841553\n",
      "batch: 647 loss: 1.5044476985931396\n",
      "batch: 648 loss: 1.496203899383545\n",
      "batch: 649 loss: 1.350126028060913\n",
      "batch: 650 loss: 1.430881142616272\n",
      "batch: 651 loss: 1.4238954782485962\n",
      "batch: 652 loss: 1.4860352277755737\n",
      "batch: 653 loss: 1.5421452522277832\n",
      "batch: 654 loss: 1.3281521797180176\n",
      "batch: 655 loss: 1.5298420190811157\n",
      "batch: 656 loss: 1.4419692754745483\n",
      "batch: 657 loss: 1.4537495374679565\n",
      "batch: 658 loss: 1.7023035287857056\n",
      "batch: 659 loss: 1.5230193138122559\n",
      "batch: 660 loss: 1.3633314371109009\n",
      "batch: 661 loss: 1.5558667182922363\n",
      "batch: 662 loss: 1.437610387802124\n",
      "batch: 663 loss: 1.45131254196167\n",
      "batch: 664 loss: 1.3634690046310425\n",
      "batch: 665 loss: 1.4804186820983887\n",
      "batch: 666 loss: 1.4355039596557617\n",
      "batch: 667 loss: 1.4215830564498901\n",
      "batch: 668 loss: 1.4021966457366943\n",
      "batch: 669 loss: 1.3766148090362549\n",
      "batch: 670 loss: 1.5252363681793213\n",
      "batch: 671 loss: 1.6981347799301147\n",
      "batch: 672 loss: 1.3846614360809326\n",
      "batch: 673 loss: 1.4238574504852295\n",
      "batch: 674 loss: 1.6173124313354492\n",
      "batch: 675 loss: 1.474874496459961\n",
      "batch: 676 loss: 1.490354061126709\n",
      "batch: 677 loss: 1.53879976272583\n",
      "batch: 678 loss: 1.490985631942749\n",
      "batch: 679 loss: 1.5785932540893555\n",
      "batch: 680 loss: 1.5146816968917847\n",
      "batch: 681 loss: 1.3868331909179688\n",
      "batch: 682 loss: 1.5445088148117065\n",
      "batch: 683 loss: 1.597910761833191\n",
      "batch: 684 loss: 1.6214895248413086\n",
      "batch: 685 loss: 1.5402235984802246\n",
      "batch: 686 loss: 1.5644829273223877\n",
      "batch: 687 loss: 1.5280059576034546\n",
      "batch: 688 loss: 1.4233421087265015\n",
      "batch: 689 loss: 1.6821237802505493\n",
      "batch: 690 loss: 1.4620448350906372\n",
      "batch: 691 loss: 1.6146248579025269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 692 loss: 1.5887483358383179\n",
      "batch: 693 loss: 1.5108063220977783\n",
      "batch: 694 loss: 1.4853379726409912\n",
      "batch: 695 loss: 1.4918365478515625\n",
      "batch: 696 loss: 1.4075357913970947\n",
      "batch: 697 loss: 1.499424695968628\n",
      "batch: 698 loss: 1.3382207155227661\n",
      "batch: 699 loss: 1.5296800136566162\n",
      "batch: 700 loss: 1.59912109375\n",
      "batch: 701 loss: 1.5875296592712402\n",
      "batch: 702 loss: 1.5580453872680664\n",
      "batch: 703 loss: 1.5948959589004517\n",
      "batch: 704 loss: 1.4713236093521118\n",
      "batch: 705 loss: 1.6038488149642944\n",
      "batch: 706 loss: 1.4878661632537842\n",
      "batch: 707 loss: 1.5405949354171753\n",
      "batch: 708 loss: 1.5652107000350952\n",
      "batch: 709 loss: 1.4030143022537231\n",
      "batch: 710 loss: 1.5743422508239746\n",
      "batch: 711 loss: 1.5732918977737427\n",
      "batch: 712 loss: 1.5674397945404053\n",
      "batch: 713 loss: 1.5326143503189087\n",
      "batch: 714 loss: 1.4193871021270752\n",
      "batch: 715 loss: 1.5051056146621704\n",
      "batch: 716 loss: 1.488375186920166\n",
      "batch: 717 loss: 1.3489508628845215\n",
      "batch: 718 loss: 1.4160475730895996\n",
      "batch: 719 loss: 1.621983528137207\n",
      "batch: 720 loss: 1.4906927347183228\n",
      "batch: 721 loss: 1.4973853826522827\n",
      "batch: 722 loss: 1.5154104232788086\n",
      "batch: 723 loss: 1.5796045064926147\n",
      "batch: 724 loss: 1.4869472980499268\n",
      "batch: 725 loss: 1.4065847396850586\n",
      "batch: 726 loss: 1.3714971542358398\n",
      "batch: 727 loss: 1.596606969833374\n",
      "batch: 728 loss: 1.5736414194107056\n",
      "batch: 729 loss: 1.4159362316131592\n",
      "batch: 730 loss: 1.4142605066299438\n",
      "batch: 731 loss: 1.5451339483261108\n",
      "batch: 732 loss: 1.3610718250274658\n",
      "batch: 733 loss: 1.4572930335998535\n",
      "batch: 734 loss: 1.3412084579467773\n",
      "batch: 735 loss: 1.4652245044708252\n",
      "batch: 736 loss: 1.645007610321045\n",
      "batch: 737 loss: 1.3489776849746704\n",
      "batch: 738 loss: 1.6730128526687622\n",
      "batch: 739 loss: 1.6455764770507812\n",
      "batch: 740 loss: 1.5616365671157837\n",
      "batch: 741 loss: 1.3920069932937622\n",
      "batch: 742 loss: 1.5847282409667969\n",
      "batch: 743 loss: 1.532105565071106\n",
      "batch: 744 loss: 1.5516109466552734\n",
      "batch: 745 loss: 1.255157709121704\n",
      "batch: 746 loss: 1.5109530687332153\n",
      "batch: 747 loss: 1.4407581090927124\n",
      "batch: 748 loss: 1.5046740770339966\n",
      "batch: 749 loss: 1.5165303945541382\n",
      "batch: 750 loss: 1.5700180530548096\n",
      "batch: 751 loss: 1.4919347763061523\n",
      "batch: 752 loss: 1.386918067932129\n",
      "batch: 753 loss: 1.3852264881134033\n",
      "batch: 754 loss: 1.5533047914505005\n",
      "batch: 755 loss: 1.3098609447479248\n",
      "batch: 756 loss: 1.4975086450576782\n",
      "batch: 757 loss: 1.4127435684204102\n",
      "batch: 758 loss: 1.497391939163208\n",
      "batch: 759 loss: 1.377942681312561\n",
      "batch: 760 loss: 1.4693189859390259\n",
      "batch: 761 loss: 1.3013901710510254\n",
      "batch: 762 loss: 1.691398024559021\n",
      "batch: 763 loss: 1.326479434967041\n",
      "batch: 764 loss: 1.586017370223999\n",
      "batch: 765 loss: 1.5160040855407715\n",
      "batch: 766 loss: 1.4715498685836792\n",
      "batch: 767 loss: 1.4192439317703247\n",
      "batch: 768 loss: 1.5395272970199585\n",
      "batch: 769 loss: 1.600478172302246\n",
      "batch: 770 loss: 1.5390958786010742\n",
      "batch: 771 loss: 1.4690872430801392\n",
      "batch: 772 loss: 1.556585431098938\n",
      "batch: 773 loss: 1.3464930057525635\n",
      "batch: 774 loss: 1.437886118888855\n",
      "batch: 775 loss: 1.504329800605774\n",
      "batch: 776 loss: 1.541080117225647\n",
      "batch: 777 loss: 1.4998676776885986\n",
      "batch: 778 loss: 1.3597147464752197\n",
      "batch: 779 loss: 1.504720687866211\n",
      "batch: 780 loss: 1.6113111972808838\n",
      "batch: 781 loss: 1.4269986152648926\n",
      "batch: 782 loss: 1.4002156257629395\n",
      "batch: 783 loss: 1.6854133605957031\n",
      "batch: 784 loss: 1.5004000663757324\n",
      "batch: 785 loss: 1.511823296546936\n",
      "batch: 786 loss: 1.431007981300354\n",
      "batch: 787 loss: 1.5645074844360352\n",
      "batch: 788 loss: 1.4581372737884521\n",
      "batch: 789 loss: 1.4177265167236328\n",
      "batch: 790 loss: 1.5707883834838867\n",
      "batch: 791 loss: 1.3556649684906006\n",
      "batch: 792 loss: 1.2992477416992188\n",
      "batch: 793 loss: 1.3217207193374634\n",
      "batch: 794 loss: 1.353798747062683\n",
      "batch: 795 loss: 1.5809129476547241\n",
      "batch: 796 loss: 1.4728814363479614\n",
      "batch: 797 loss: 1.7145071029663086\n",
      "batch: 798 loss: 1.4760674238204956\n",
      "batch: 799 loss: 1.6027600765228271\n",
      "batch: 800 loss: 1.395066738128662\n",
      "batch: 801 loss: 1.5581440925598145\n",
      "batch: 802 loss: 1.3845295906066895\n",
      "batch: 803 loss: 1.5320156812667847\n",
      "batch: 804 loss: 1.4319748878479004\n",
      "batch: 805 loss: 1.4867596626281738\n",
      "batch: 806 loss: 1.5292630195617676\n",
      "batch: 807 loss: 1.517617106437683\n",
      "batch: 808 loss: 1.3711718320846558\n",
      "batch: 809 loss: 1.4243677854537964\n",
      "batch: 810 loss: 1.4958066940307617\n",
      "batch: 811 loss: 1.5266692638397217\n",
      "batch: 812 loss: 1.4753623008728027\n",
      "batch: 813 loss: 1.5200990438461304\n",
      "batch: 814 loss: 1.56154465675354\n",
      "batch: 815 loss: 1.3697867393493652\n",
      "batch: 816 loss: 1.4941569566726685\n",
      "batch: 817 loss: 1.5448808670043945\n",
      "batch: 818 loss: 1.5260353088378906\n",
      "batch: 819 loss: 1.4165401458740234\n",
      "batch: 820 loss: 1.486222505569458\n",
      "batch: 821 loss: 1.4700284004211426\n",
      "batch: 822 loss: 1.4851677417755127\n",
      "batch: 823 loss: 1.3303921222686768\n",
      "batch: 824 loss: 1.4123698472976685\n",
      "batch: 825 loss: 1.5522161722183228\n",
      "batch: 826 loss: 1.5376168489456177\n",
      "batch: 827 loss: 1.4203194379806519\n",
      "batch: 828 loss: 1.324175238609314\n",
      "batch: 829 loss: 1.6262065172195435\n",
      "batch: 830 loss: 1.470367193222046\n",
      "batch: 831 loss: 1.4531055688858032\n",
      "batch: 832 loss: 1.5959923267364502\n",
      "batch: 833 loss: 1.4130462408065796\n",
      "batch: 834 loss: 1.616782546043396\n",
      "batch: 835 loss: 1.5041167736053467\n",
      "batch: 836 loss: 1.425207257270813\n",
      "batch: 837 loss: 1.5534367561340332\n",
      "batch: 838 loss: 1.5260474681854248\n",
      "batch: 839 loss: 1.4547367095947266\n",
      "batch: 840 loss: 1.3663833141326904\n",
      "batch: 841 loss: 1.5801199674606323\n",
      "batch: 842 loss: 1.4627060890197754\n",
      "batch: 843 loss: 1.4300603866577148\n",
      "batch: 844 loss: 1.5956770181655884\n",
      "batch: 845 loss: 1.5339573621749878\n",
      "batch: 846 loss: 1.4146392345428467\n",
      "batch: 847 loss: 1.3510364294052124\n",
      "batch: 848 loss: 1.4407312870025635\n",
      "batch: 849 loss: 1.5894060134887695\n",
      "batch: 850 loss: 1.393306016921997\n",
      "batch: 851 loss: 1.3969813585281372\n",
      "batch: 852 loss: 1.4617741107940674\n",
      "batch: 853 loss: 1.4393081665039062\n",
      "batch: 854 loss: 1.5811740159988403\n",
      "batch: 855 loss: 1.2353876829147339\n",
      "batch: 856 loss: 1.460410237312317\n",
      "batch: 857 loss: 1.2081537246704102\n",
      "batch: 858 loss: 1.4027611017227173\n",
      "batch: 859 loss: 1.5639973878860474\n",
      "batch: 860 loss: 1.2872776985168457\n",
      "batch: 861 loss: 1.5145623683929443\n",
      "batch: 862 loss: 1.6785131692886353\n",
      "batch: 863 loss: 1.5108853578567505\n",
      "batch: 864 loss: 1.398483395576477\n",
      "batch: 865 loss: 1.35573148727417\n",
      "batch: 866 loss: 1.5970861911773682\n",
      "batch: 867 loss: 1.5140693187713623\n",
      "batch: 868 loss: 1.5879607200622559\n",
      "batch: 869 loss: 1.4510496854782104\n",
      "batch: 870 loss: 1.4994308948516846\n",
      "batch: 871 loss: 1.414063572883606\n",
      "batch: 872 loss: 1.4366742372512817\n",
      "batch: 873 loss: 1.7956708669662476\n",
      "batch: 874 loss: 1.520761489868164\n",
      "batch: 875 loss: 1.454397439956665\n",
      "batch: 876 loss: 1.394092082977295\n",
      "batch: 877 loss: 1.3871458768844604\n",
      "batch: 878 loss: 1.4063668251037598\n",
      "batch: 879 loss: 1.3828990459442139\n",
      "batch: 880 loss: 1.2879502773284912\n",
      "batch: 881 loss: 1.5320230722427368\n",
      "batch: 882 loss: 1.4705373048782349\n",
      "batch: 883 loss: 1.4275120496749878\n",
      "batch: 884 loss: 1.4471909999847412\n",
      "batch: 885 loss: 1.4145119190216064\n",
      "batch: 886 loss: 1.443647861480713\n",
      "batch: 887 loss: 1.3985984325408936\n",
      "batch: 888 loss: 1.5734409093856812\n",
      "batch: 889 loss: 1.5001388788223267\n",
      "batch: 890 loss: 1.4330382347106934\n",
      "batch: 891 loss: 1.5430190563201904\n",
      "batch: 892 loss: 1.742243766784668\n",
      "batch: 893 loss: 1.56092369556427\n",
      "batch: 894 loss: 1.5481693744659424\n",
      "batch: 895 loss: 1.5281848907470703\n",
      "batch: 896 loss: 1.462843656539917\n",
      "batch: 897 loss: 1.4299743175506592\n",
      "batch: 898 loss: 1.5770447254180908\n",
      "batch: 899 loss: 1.3643288612365723\n",
      "batch: 900 loss: 1.5121172666549683\n",
      "batch: 901 loss: 1.3592395782470703\n",
      "batch: 902 loss: 1.4652605056762695\n",
      "batch: 903 loss: 1.4514296054840088\n",
      "batch: 904 loss: 1.5665801763534546\n",
      "batch: 905 loss: 1.444893479347229\n",
      "batch: 906 loss: 1.654132604598999\n",
      "batch: 907 loss: 1.496524691581726\n",
      "batch: 908 loss: 1.5430190563201904\n",
      "batch: 909 loss: 1.68247389793396\n",
      "batch: 910 loss: 1.4623218774795532\n",
      "batch: 911 loss: 1.3916144371032715\n",
      "batch: 912 loss: 1.2728904485702515\n",
      "batch: 913 loss: 1.6359950304031372\n",
      "batch: 914 loss: 1.2779282331466675\n",
      "batch: 915 loss: 1.4905115365982056\n",
      "batch: 916 loss: 1.3077800273895264\n",
      "batch: 917 loss: 1.6262218952178955\n",
      "batch: 918 loss: 1.398688554763794\n",
      "batch: 919 loss: 1.2798287868499756\n",
      "batch: 920 loss: 1.4362895488739014\n",
      "batch: 921 loss: 1.313562035560608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 922 loss: 1.3013238906860352\n",
      "batch: 923 loss: 1.5126008987426758\n",
      "batch: 924 loss: 1.5326192378997803\n",
      "batch: 925 loss: 1.5327996015548706\n",
      "batch: 926 loss: 1.308176875114441\n",
      "batch: 927 loss: 1.424000859260559\n",
      "batch: 928 loss: 1.3948500156402588\n",
      "batch: 929 loss: 1.4289954900741577\n",
      "batch: 930 loss: 1.4064608812332153\n",
      "batch: 931 loss: 1.64668869972229\n",
      "batch: 932 loss: 1.4102280139923096\n",
      "batch: 933 loss: 1.45430326461792\n",
      "batch: 934 loss: 1.4368939399719238\n",
      "batch: 935 loss: 1.4146747589111328\n",
      "batch: 936 loss: 1.4241747856140137\n",
      "batch: 937 loss: 1.3596975803375244\n",
      "batch: 938 loss: 1.4374370574951172\n",
      "batch: 939 loss: 1.6585144996643066\n",
      "batch: 940 loss: 1.4785914421081543\n",
      "batch: 941 loss: 1.5779478549957275\n",
      "batch: 942 loss: 1.6825058460235596\n",
      "batch: 943 loss: 1.2985413074493408\n",
      "batch: 944 loss: 1.3762263059616089\n",
      "batch: 945 loss: 1.2789950370788574\n",
      "batch: 946 loss: 1.3503968715667725\n",
      "batch: 947 loss: 1.446842908859253\n",
      "batch: 948 loss: 1.4212195873260498\n",
      "batch: 949 loss: 1.508620262145996\n",
      "batch: 950 loss: 1.2904510498046875\n",
      "batch: 951 loss: 1.4649429321289062\n",
      "batch: 952 loss: 1.5656626224517822\n",
      "batch: 953 loss: 1.4118962287902832\n",
      "batch: 954 loss: 1.4094769954681396\n",
      "batch: 955 loss: 1.6425179243087769\n",
      "batch: 956 loss: 1.6130294799804688\n",
      "batch: 957 loss: 1.3926749229431152\n",
      "batch: 958 loss: 1.4951759576797485\n",
      "batch: 959 loss: 1.5433377027511597\n",
      "batch: 960 loss: 1.4632794857025146\n",
      "batch: 961 loss: 1.605639934539795\n",
      "batch: 962 loss: 1.4227650165557861\n",
      "batch: 963 loss: 1.4211150407791138\n",
      "batch: 964 loss: 1.4748139381408691\n",
      "batch: 965 loss: 1.4573731422424316\n",
      "batch: 966 loss: 1.4206938743591309\n",
      "batch: 967 loss: 1.3578882217407227\n",
      "batch: 968 loss: 1.3920469284057617\n",
      "batch: 969 loss: 1.428032398223877\n",
      "batch: 970 loss: 1.5679129362106323\n",
      "batch: 971 loss: 1.434713363647461\n",
      "batch: 972 loss: 1.3700685501098633\n",
      "batch: 973 loss: 1.6016626358032227\n",
      "batch: 974 loss: 1.3472483158111572\n",
      "batch: 975 loss: 1.3801862001419067\n",
      "batch: 976 loss: 1.5080304145812988\n",
      "batch: 977 loss: 1.2513034343719482\n",
      "batch: 978 loss: 1.291258454322815\n",
      "batch: 979 loss: 1.2465243339538574\n",
      "batch: 980 loss: 1.4251407384872437\n",
      "batch: 981 loss: 1.3164911270141602\n",
      "batch: 982 loss: 1.396003007888794\n",
      "batch: 983 loss: 1.334121823310852\n",
      "batch: 984 loss: 1.6257684230804443\n",
      "batch: 985 loss: 1.3784621953964233\n",
      "batch: 986 loss: 1.3643789291381836\n",
      "batch: 987 loss: 1.5628963708877563\n",
      "batch: 988 loss: 1.4569075107574463\n",
      "batch: 989 loss: 1.4726701974868774\n",
      "batch: 990 loss: 1.4172582626342773\n",
      "batch: 991 loss: 1.4358664751052856\n",
      "batch: 992 loss: 1.5773242712020874\n",
      "batch: 993 loss: 1.4861128330230713\n",
      "batch: 994 loss: 1.4459726810455322\n",
      "batch: 995 loss: 1.5095500946044922\n",
      "batch: 996 loss: 1.4774047136306763\n",
      "batch: 997 loss: 1.4493117332458496\n",
      "batch: 998 loss: 1.4378989934921265\n",
      "batch: 999 loss: 1.345624566078186\n",
      "batch: 1000 loss: 1.353837251663208\n",
      "batch: 1001 loss: 1.3984742164611816\n",
      "batch: 1002 loss: 1.5285009145736694\n",
      "batch: 1003 loss: 1.3973193168640137\n",
      "batch: 1004 loss: 1.2614272832870483\n",
      "batch: 1005 loss: 1.4227349758148193\n",
      "batch: 1006 loss: 1.2192020416259766\n",
      "batch: 1007 loss: 1.4348351955413818\n",
      "batch: 1008 loss: 1.4464999437332153\n",
      "batch: 1009 loss: 1.3472883701324463\n",
      "batch: 1010 loss: 1.4650565385818481\n",
      "batch: 1011 loss: 1.4441472291946411\n",
      "batch: 1012 loss: 1.3775314092636108\n",
      "batch: 1013 loss: 1.3842765092849731\n",
      "batch: 1014 loss: 1.4529354572296143\n",
      "batch: 1015 loss: 1.3975549936294556\n",
      "batch: 1016 loss: 1.4435722827911377\n",
      "batch: 1017 loss: 1.4300118684768677\n",
      "batch: 1018 loss: 1.2740000486373901\n",
      "batch: 1019 loss: 1.3725545406341553\n",
      "batch: 1020 loss: 1.3980906009674072\n",
      "batch: 1021 loss: 1.3406833410263062\n",
      "batch: 1022 loss: 1.403134822845459\n",
      "batch: 1023 loss: 1.346327543258667\n",
      "batch: 1024 loss: 1.310215711593628\n",
      "batch: 1025 loss: 1.5378990173339844\n",
      "batch: 1026 loss: 1.4477732181549072\n",
      "batch: 1027 loss: 1.344939112663269\n",
      "batch: 1028 loss: 1.4793670177459717\n",
      "batch: 1029 loss: 1.4153952598571777\n",
      "batch: 1030 loss: 1.407099962234497\n",
      "batch: 1031 loss: 1.3689618110656738\n",
      "batch: 1032 loss: 1.3824586868286133\n",
      "batch: 1033 loss: 1.3570270538330078\n",
      "batch: 1034 loss: 1.560575008392334\n",
      "batch: 1035 loss: 1.4954636096954346\n",
      "batch: 1036 loss: 1.3315834999084473\n",
      "batch: 1037 loss: 1.3969359397888184\n",
      "batch: 1038 loss: 1.405250906944275\n",
      "batch: 1039 loss: 1.4227484464645386\n",
      "batch: 1040 loss: 1.2986944913864136\n",
      "batch: 1041 loss: 1.4414846897125244\n",
      "batch: 1042 loss: 1.4388982057571411\n",
      "batch: 1043 loss: 1.361096739768982\n",
      "batch: 1044 loss: 1.3077630996704102\n",
      "batch: 1045 loss: 1.2970867156982422\n",
      "batch: 1046 loss: 1.448204755783081\n",
      "batch: 1047 loss: 1.2780859470367432\n",
      "batch: 1048 loss: 1.4260520935058594\n",
      "batch: 1049 loss: 1.3275920152664185\n",
      "batch: 1050 loss: 1.3128538131713867\n",
      "batch: 1051 loss: 1.3416022062301636\n",
      "batch: 1052 loss: 1.388484001159668\n",
      "batch: 1053 loss: 1.4409550428390503\n",
      "batch: 1054 loss: 1.5575467348098755\n",
      "batch: 1055 loss: 1.3966009616851807\n",
      "batch: 1056 loss: 1.361119270324707\n",
      "batch: 1057 loss: 1.2360552549362183\n",
      "batch: 1058 loss: 1.4196935892105103\n",
      "batch: 1059 loss: 1.3861576318740845\n",
      "batch: 1060 loss: 1.4236418008804321\n",
      "batch: 1061 loss: 1.360376238822937\n",
      "batch: 1062 loss: 1.4266663789749146\n",
      "batch: 1063 loss: 1.3567447662353516\n",
      "batch: 1064 loss: 1.375677227973938\n",
      "batch: 1065 loss: 1.2775872945785522\n",
      "batch: 1066 loss: 1.3953289985656738\n",
      "batch: 1067 loss: 1.6264444589614868\n",
      "batch: 1068 loss: 1.327494740486145\n",
      "batch: 1069 loss: 1.2258890867233276\n",
      "batch: 1070 loss: 1.2369095087051392\n",
      "batch: 1071 loss: 1.361878752708435\n",
      "batch: 1072 loss: 1.382093071937561\n",
      "batch: 1073 loss: 1.4490288496017456\n",
      "batch: 1074 loss: 1.4118244647979736\n",
      "batch: 1075 loss: 1.337545394897461\n",
      "batch: 1076 loss: 1.3971202373504639\n",
      "batch: 1077 loss: 1.3437647819519043\n",
      "batch: 1078 loss: 1.2356879711151123\n",
      "batch: 1079 loss: 1.3739675283432007\n",
      "batch: 1080 loss: 1.568984031677246\n",
      "batch: 1081 loss: 1.3637571334838867\n",
      "batch: 1082 loss: 1.383743405342102\n",
      "batch: 1083 loss: 1.3042590618133545\n",
      "batch: 1084 loss: 1.4197134971618652\n",
      "batch: 1085 loss: 1.491112232208252\n",
      "batch: 1086 loss: 1.423581838607788\n",
      "batch: 1087 loss: 1.3818411827087402\n",
      "batch: 1088 loss: 1.431434988975525\n",
      "batch: 1089 loss: 1.238351583480835\n",
      "batch: 1090 loss: 1.4562972784042358\n",
      "batch: 1091 loss: 1.5320408344268799\n",
      "batch: 1092 loss: 1.7183680534362793\n",
      "batch: 1093 loss: 1.5495001077651978\n",
      "batch: 1094 loss: 1.437532663345337\n",
      "batch: 1095 loss: 1.3631502389907837\n",
      "batch: 1096 loss: 1.3922340869903564\n",
      "batch: 1097 loss: 1.4386225938796997\n",
      "batch: 1098 loss: 1.453258752822876\n",
      "batch: 1099 loss: 1.3088816404342651\n",
      "batch: 1100 loss: 1.4458072185516357\n",
      "batch: 1101 loss: 1.4281339645385742\n",
      "batch: 1102 loss: 1.4455010890960693\n",
      "batch: 1103 loss: 1.4798235893249512\n",
      "batch: 1104 loss: 1.3166857957839966\n",
      "batch: 1105 loss: 1.5136137008666992\n",
      "batch: 1106 loss: 1.4247905015945435\n",
      "batch: 1107 loss: 1.3434123992919922\n",
      "batch: 1108 loss: 1.420424461364746\n",
      "batch: 1109 loss: 1.293550968170166\n",
      "batch: 1110 loss: 1.3515170812606812\n",
      "batch: 1111 loss: 1.2413759231567383\n",
      "batch: 1112 loss: 1.5559495687484741\n",
      "batch: 1113 loss: 1.548216700553894\n",
      "batch: 1114 loss: 1.3492295742034912\n",
      "batch: 1115 loss: 1.610613465309143\n",
      "batch: 1116 loss: 1.3914439678192139\n",
      "batch: 1117 loss: 1.3356692790985107\n",
      "batch: 1118 loss: 1.3607114553451538\n",
      "batch: 1119 loss: 1.5375897884368896\n",
      "batch: 1120 loss: 1.3175959587097168\n",
      "batch: 1121 loss: 1.384641408920288\n",
      "batch: 1122 loss: 1.5579447746276855\n",
      "batch: 1123 loss: 1.3824771642684937\n",
      "batch: 1124 loss: 1.4609218835830688\n",
      "batch: 1125 loss: 1.381919026374817\n",
      "batch: 1126 loss: 1.4698631763458252\n",
      "batch: 1127 loss: 1.4118826389312744\n",
      "batch: 1128 loss: 1.3948328495025635\n",
      "batch: 1129 loss: 1.4902217388153076\n",
      "batch: 1130 loss: 1.509188175201416\n",
      "batch: 1131 loss: 1.3947558403015137\n",
      "batch: 1132 loss: 1.3323694467544556\n",
      "batch: 1133 loss: 1.2610139846801758\n",
      "batch: 1134 loss: 1.354048490524292\n",
      "batch: 1135 loss: 1.4286895990371704\n",
      "batch: 1136 loss: 1.3925931453704834\n",
      "batch: 1137 loss: 1.5865298509597778\n",
      "batch: 1138 loss: 1.2237768173217773\n",
      "batch: 1139 loss: 1.4459433555603027\n",
      "batch: 1140 loss: 1.510212779045105\n",
      "batch: 1141 loss: 1.2002660036087036\n",
      "batch: 1142 loss: 1.480530023574829\n",
      "batch: 1143 loss: 1.4636270999908447\n",
      "batch: 1144 loss: 1.373659372329712\n",
      "batch: 1145 loss: 1.4001517295837402\n",
      "batch: 1146 loss: 1.3139454126358032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1147 loss: 1.421688437461853\n",
      "batch: 1148 loss: 1.498097538948059\n",
      "batch: 1149 loss: 1.5180776119232178\n",
      "batch: 1150 loss: 1.3100042343139648\n",
      "batch: 1151 loss: 1.2252395153045654\n",
      "batch: 1152 loss: 1.3349628448486328\n",
      "batch: 1153 loss: 1.4160759449005127\n",
      "batch: 1154 loss: 1.4013762474060059\n",
      "batch: 1155 loss: 1.2276513576507568\n",
      "batch: 1156 loss: 1.3975234031677246\n",
      "batch: 1157 loss: 1.349096417427063\n",
      "batch: 1158 loss: 1.3523156642913818\n",
      "batch: 1159 loss: 1.3205373287200928\n",
      "batch: 1160 loss: 1.3386040925979614\n",
      "batch: 1161 loss: 1.4198421239852905\n",
      "batch: 1162 loss: 1.429045557975769\n",
      "batch: 1163 loss: 1.3452585935592651\n",
      "batch: 1164 loss: 1.4707612991333008\n",
      "batch: 1165 loss: 1.451441764831543\n",
      "batch: 1166 loss: 1.305570363998413\n",
      "batch: 1167 loss: 1.4362907409667969\n",
      "batch: 1168 loss: 1.321254014968872\n",
      "batch: 1169 loss: 1.441792607307434\n",
      "batch: 1170 loss: 1.465566873550415\n",
      "batch: 1171 loss: 1.2504479885101318\n",
      "batch: 1172 loss: 1.5173866748809814\n",
      "batch: 1173 loss: 1.3527168035507202\n",
      "batch: 1174 loss: 1.4366331100463867\n",
      "batch: 1175 loss: 1.4533408880233765\n",
      "batch: 1176 loss: 1.4673395156860352\n",
      "batch: 1177 loss: 1.37245774269104\n",
      "batch: 1178 loss: 1.406211256980896\n",
      "batch: 1179 loss: 1.420152187347412\n",
      "batch: 1180 loss: 1.308097243309021\n",
      "batch: 1181 loss: 1.5047036409378052\n",
      "batch: 1182 loss: 1.3265646696090698\n",
      "batch: 1183 loss: 1.2483845949172974\n",
      "batch: 1184 loss: 1.2028650045394897\n",
      "batch: 1185 loss: 1.4813112020492554\n",
      "batch: 1186 loss: 1.2861111164093018\n",
      "batch: 1187 loss: 1.325932502746582\n",
      "batch: 1188 loss: 1.4905825853347778\n",
      "batch: 1189 loss: 1.26669442653656\n",
      "batch: 1190 loss: 1.4888943433761597\n",
      "batch: 1191 loss: 1.3400639295578003\n",
      "batch: 1192 loss: 1.1911051273345947\n",
      "batch: 1193 loss: 1.2859246730804443\n",
      "batch: 1194 loss: 1.38717520236969\n",
      "batch: 1195 loss: 1.2490068674087524\n",
      "batch: 1196 loss: 1.4891939163208008\n",
      "batch: 1197 loss: 1.5042179822921753\n",
      "batch: 1198 loss: 1.4588912725448608\n",
      "batch: 1199 loss: 1.4339016675949097\n",
      "batch: 1200 loss: 1.5934453010559082\n",
      "batch: 1201 loss: 1.3819124698638916\n",
      "batch: 1202 loss: 1.3433316946029663\n",
      "batch: 1203 loss: 1.1645352840423584\n",
      "batch: 1204 loss: 1.406880259513855\n",
      "batch: 1205 loss: 1.4060542583465576\n",
      "batch: 1206 loss: 1.2223237752914429\n",
      "batch: 1207 loss: 1.3012025356292725\n",
      "batch: 1208 loss: 1.5715538263320923\n",
      "batch: 1209 loss: 1.3554106950759888\n",
      "batch: 1210 loss: 1.3883318901062012\n",
      "batch: 1211 loss: 1.2017457485198975\n",
      "batch: 1212 loss: 1.2808722257614136\n",
      "batch: 1213 loss: 1.39841890335083\n",
      "batch: 1214 loss: 1.4821178913116455\n",
      "batch: 1215 loss: 1.4507417678833008\n",
      "batch: 1216 loss: 1.5710055828094482\n",
      "batch: 1217 loss: 1.3869333267211914\n",
      "batch: 1218 loss: 1.3299137353897095\n",
      "batch: 1219 loss: 1.3822083473205566\n",
      "batch: 1220 loss: 1.2448694705963135\n",
      "batch: 1221 loss: 1.4983150959014893\n",
      "batch: 1222 loss: 1.4053629636764526\n",
      "batch: 1223 loss: 1.3600542545318604\n",
      "batch: 1224 loss: 1.5078619718551636\n",
      "batch: 1225 loss: 1.240289568901062\n",
      "batch: 1226 loss: 1.4528228044509888\n",
      "batch: 1227 loss: 1.3415216207504272\n",
      "batch: 1228 loss: 1.3350404500961304\n",
      "batch: 1229 loss: 1.4921233654022217\n",
      "batch: 1230 loss: 1.489661455154419\n",
      "batch: 1231 loss: 1.384827971458435\n",
      "batch: 1232 loss: 1.4603416919708252\n",
      "batch: 1233 loss: 1.2601182460784912\n",
      "batch: 1234 loss: 1.4946043491363525\n",
      "batch: 1235 loss: 1.2251099348068237\n",
      "batch: 1236 loss: 1.3878846168518066\n",
      "batch: 1237 loss: 1.424793004989624\n",
      "batch: 1238 loss: 1.4903606176376343\n",
      "batch: 1239 loss: 1.2021169662475586\n",
      "batch: 1240 loss: 1.3078497648239136\n",
      "batch: 1241 loss: 1.3071521520614624\n",
      "batch: 1242 loss: 1.3512139320373535\n",
      "batch: 1243 loss: 1.2660305500030518\n",
      "batch: 1244 loss: 1.2939485311508179\n",
      "batch: 1245 loss: 1.1750426292419434\n",
      "batch: 1246 loss: 1.3026230335235596\n",
      "batch: 1247 loss: 1.4517998695373535\n",
      "batch: 1248 loss: 1.339011788368225\n",
      "batch: 1249 loss: 1.3224273920059204\n",
      "batch: 1250 loss: 1.352092981338501\n",
      "batch: 1251 loss: 1.3698543310165405\n",
      "batch: 1252 loss: 1.3283894062042236\n",
      "batch: 1253 loss: 1.326230525970459\n",
      "batch: 1254 loss: 1.4427748918533325\n",
      "batch: 1255 loss: 1.0746409893035889\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./_cifar10_cnn.log\",'a') \n",
    "n_epoch = epoch\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    tf.train.start_queue_runners()\n",
    "    writer = tf.summary.FileWriter(\"./graphs/cifar10\",sess.graph)\n",
    "    sess.run(init)\n",
    "    for step in range(n_epoch):\n",
    "        image_batch,label_batch = sess.run([images_train,labels_train])\n",
    "        _,loss_value = sess.run([optimizer,loss],feed_dict={image_holder:image_batch,y_:label_batch})\n",
    "        print(\"batch: {0} loss: {1}\".format(step,loss_value))\n",
    "        f.write(\"batch: {0} loss: {1}\".format(step,loss_value))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    print(\"optimizer finished\")\n",
    "    f.write(\"optimizer finished\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #测试模型\n",
    "  \n",
    "    top_k_op = tf.nn.in_top_k(logits,y_,1)\n",
    "    num_examples = 10000\n",
    "    import math\n",
    "    num_iter = int(match.ceil(num_examples/batch_size))\n",
    "    true_count = 0\n",
    "    total_sample_count = num_iter*batch_size\n",
    "    step=0\n",
    "    while step < num_iter:\n",
    "        image_batch,label_batch = sess.run([images_test,labels_test])\n",
    "        pre = sess.run([top_k_op],feed_dict={image_holder:image_batch,y_:label_batch})\n",
    "        true_count += np.sum(pre)\n",
    "        step += 1\n",
    "    pre_avrage = true_count/total_sample_count\n",
    "    print(\"accracy: {0}\".format(pre_avrage))\n",
    "    f.write(\"accracy: {0}\".format(pre_avrage))\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    writer.close()\n",
    "   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
