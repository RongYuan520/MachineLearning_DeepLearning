http://blog.csdn.net/sxf1061926959/article/details/66976356?locationNum=9&fps=1

线性回归算法用来做什么的？
 -一般情况下做连续值的预测的，如预测房价，股票
 -对于到机器学习中，属于监督学习，即给定一个情况的信息，要告诉它属于什么类型
 -需要学习映射：f:X->Y，X和Y存在现行相关性，如身高和体重，房价与房子面积，在坐标系中可以用一条直线来拟合，y=ax+b

考虑多个变量的情形
 把y用关于x的函数来描述，就是hu(x)=u0+u1x1+u2x2,一组训练集[x1,x2,x3,,,,,,xn]，一组系数[u0,u1,u2,,,,,un]

损失函数
 我们要找到最好的权重/系数u=[u0,u1,,,,un],怎么衡量是最好？就是我们在有权重的情况下，看看权重拟合的直线离训练的每个点距离的和是否最小
 假设我们x到Y的映射为hu(x)定义损失函数为：
							J(U0,U1,,,Un)=1/2n'''''''''''''''
 最小化损失函数
	横坐标为权重u,纵坐标为J（u）,表现出来是个凸函数，就是有坡度，有最低点。如果是一个特征，那么就一个系数，是个碗形
	方法1：最小二乘法，求极值
	方法2：梯度下降：如同下山，找准方向（斜率），每次迈出一小步，直至山底，每次更新梯度u1=u1-a*d(J)/d(u1),a为步长，即学习率
		   学习率与梯度下降：目前我们自己设置0.05
			-a小，收敛慢
			-a大，到不了最低点

回归与过拟合和欠拟合
	过拟合问题：如果我们有特别多的特征，我们假设函数曲线可以和原始数据拟合的特别好，波动很大，曲折很多，但丧失了一般性，失去了泛化能力，从而导致对新的待预测样本，预测效果差。其实质是参数太多了，或者说特征太多了。
	欠拟合问题：不论怎么调整模型直线，总是和某些点距离比较大。
 

回归与正则化
	为了解决过拟合的情况，我们使用正则化技术或者增大样本个数来消除过拟合
	
	例如：
		正确的模型：y = u1x1 + u2x2
		过拟合的模型：y = u1x1 + u2x2 + u3x3 + u4x4
		通过模型表达式我们可以看出，过拟合的模型多了两项：u3和u4,所以我们如果使得u3和u4等于0，就会消除过拟合，我们给loss加上正则化项，当我们优化loss的时候，
		loss = 以前的loss + 正则化现，loss需要变小，那么以前的loss和正则化项都需要变小，正则化变小，那么我们的权重也就变小了。


		两外一种解释：
					y = u1x1 + u2x2 + u3x3
					你需要解这个方程，那么需要3个点才能求解出权重，这个就相当于过拟合了。
					如果给它2个点，相当于欠拟合了。
					如果给他4个点，那么权重需要尽可能满足这4个点，所以他就相当于需要优化，使得每个点的误差都很小。

					我们如果想要解决过拟合问题，那么我们只能给更多的点，就可以消除过拟合，相当于增大样本个数。
					如果我们只有3个点，必须解决过拟合，那么我们只能去掉一个u，即去掉一个特征，相当于我们开始给的特征太多了，所以我们需要消除一个特征，这个就相当于正则化					，正则化说到底他就是使得u1或者2或者3其中一个变的很小了，相当于去掉了该特征。

