线性回归拟合y=wx+b这条直线，而逻辑回归是为了拟合一个判定边界

sigmoid函数可以把任意连续的值映射到[0,1]之间，
用线性回归解决分类问题，发现会出现噪声样本，因此利用sigmoid函数，将连续值问题映射到[0,1]之间成为概率问题


判定边界：判定边界就是z=0,z就是w1x1+wx2=0的边界，当z>0,z<0,刚好在边界的一侧，赶赴好是sigmoid函数对应大于0.5，小于0.5
	线性判定边界：例如一条直线上下测
	非线性判定边界：例如圆里外测

怎么判别我们选择的边界是不是好呢？-----损失函数
	逻辑回归里的损失函数不能用以前的误差平方和了，因为我们为了求损失函数最小值，要是用凸函数，用误差平方和的方法不是凸函数，无法球极值
	因此在逻辑回归中，用另一种损失函数log函数，不能忘记正则化项。正则化项可以使边界曲线更加平滑，消除高次项特征的影响，逻辑回归损失函数表达式：
			J(u)=[] + lamda
	同样用梯度下降法计算损失函数最小值，从而确定一组最优参数/权重，进而确定决策边界



二分类与多分类
	one-vs-rest:多个分类器，每个分类器只能区分是否其中一种分类

LR应用
	今日头条在一天广告赚1000w+的新闻app排序基线是LR
	点击率预估（CTR）/推荐系统
	电商搜索基线版本也是LR
	电商的购物搭配推荐也用了大量的LR
	
LR优点
	添加特征很简单
	由于的道德是概率。可以进行概率比较

应用经验：
	样本平衡，特征处理，算法调优
